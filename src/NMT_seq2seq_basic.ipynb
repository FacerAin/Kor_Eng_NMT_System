{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python37464bitvenvvenv1c8fbc684c88465593970c8fb2f22879",
      "display_name": "Python 3.7.4 64-bit ('venv': venv)"
    },
    "colab": {
      "name": "NMT_seq2seq_basic.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5hhSZna0zh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "febda62f-37b1-4f0f-cfe6-14cf848be404"
      },
      "source": [
        "!git clone https://github.com/FacerAin/Kor_Eng_NMT_System.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Kor_Eng_NMT_System'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 10 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dpGBb-X0d68",
        "colab_type": "text"
      },
      "source": [
        "## NMT_seq2seq_basic\n",
        "\n",
        "Character Level NMT based on Simple Seq2Seq Model Without Attention Mechanism \n",
        "\n",
        "Refer to https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V27ojmrJ0d6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIyZgSr0d7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameter Setting\n",
        "dataset_dir_path = '../dataset/kor-eng/'\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfBWDMn0d7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "d7BvUjss0d7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "794fe83e-a5b1-4abb-dba1-10618a421902"
      },
      "source": [
        "# Read Dataset and Preprocessing\n",
        "with open(dataset_dir_path+\"kor.txt\", \"r\", encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for line in lines[:len(lines)-1]:\n",
        "        split_line = line.split('\\t')\n",
        "        input_texts.append(split_line[0])\n",
        "        target_texts.append('\\t' + split_line[1]+ '\\n')\n",
        "        for char_item in split_line[0]:\n",
        "            input_characters.add(char_item)\n",
        "        for char_item in split_line[1]:\n",
        "            target_characters.add(char_item)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-60187010ae56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read Dataset and Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"kor.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msplit_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/kor-eng/kor.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OutOaTNo0d7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "92891512-b385-4c7a-edb4-e0a56669b8fb"
      },
      "source": [
        "input_characters = sorted(input_characters)\n",
        "target_characters = sorted(target_characters)\n",
        "\n",
        "encoder_token_num = len(input_characters)\n",
        "target_token_num = len(target_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(item) for item in input_texts])\n",
        "max_decoder_seq_length = max([len(item) for item in target_texts])\n",
        "\n",
        "encoder_token_index_dict = dict([(char_item, i) for i, char_item in enumerate(input_characters)])\n",
        "target_token_index_dict = dict([(char_item, i) for i, char_item in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, encoder_token_num), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, target_token_num), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, target_token_num), dtype='float32')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8c9f6eb7e9b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget_token_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_characters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmax_encoder_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmax_decoder_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5Y0rJKos0d7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,(input_text, target_text) in enumerate(zip(input_texts,target_texts)):\n",
        "    for j, char_item in enumerate(input_text):\n",
        "        encoder_input_data[i, j, encoder_token_index_dict[char_item]] = 1.\n",
        "    encoder_input_data[i, j+1:, encoder_token_index_dict[\" \"]] = 1.\n",
        "    for j, char_item in enumerate(target_text):\n",
        "        decoder_input_data[i, j, target_token_index_dict[char_item]] = 1.\n",
        "        if j > 0:\n",
        "            decoder_target_data[i , j-1, target_token_index_dict[char_item]] = 1.\n",
        "        decoder_input_data[i, j+1: ,target_token_index[\" \"]] = 1.\n",
        "        decoder_target_data[i, j:, target_token_index[\" \"]] = 1.\n",
        "\n",
        "        \n",
        "            \n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQgkj_EO0d7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}