{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python37464bitvenvvenv1c8fbc684c88465593970c8fb2f22879",
      "display_name": "Python 3.7.4 64-bit ('venv': venv)"
    },
    "colab": {
      "name": "NMT_seq2seq_basic.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5hhSZna0zh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "849b4e4a-6f47-47a6-8179-6cecb552b2ae"
      },
      "source": [
        "#For Colab ENV\n",
        "from google.colab import drive \n",
        "drive.mount('/gdrive')\n",
        "!git clone https://github.com/FacerAin/Kor_Eng_NMT_System.git '/gdrive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "Cloning into '/gdrive/My Drive/Colab Notebooks'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 18 (delta 2), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dpGBb-X0d68",
        "colab_type": "text"
      },
      "source": [
        "## NMT_seq2seq_basic\n",
        "\n",
        "Character Level NMT based on Simple Seq2Seq Model Without Attention Mechanism \n",
        "\n",
        "Refer to https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V27ojmrJ0d6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIyZgSr0d7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameter Setting\n",
        "dataset_dir_path = '../dataset/kor-eng/'\n",
        "batch_size = 64\n",
        "latent_dim = 256\n",
        "epochs = 50\n",
        "num_samples = 2500"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfBWDMn0d7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "d7BvUjss0d7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read Dataset and Preprocessing\n",
        "with open(dataset_dir_path+\"kor.txt\", \"r\", encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for line in lines[:min(num_samples, len(lines)-2)]:#To Except Last Line\n",
        "        split_line = line.split('\\t')\n",
        "        input_text = split_line[0]\n",
        "        target_text = '\\t' + split_line[1]+ '\\n' #Using '^' to start tag, '&' to end tag\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for char_item in input_text:\n",
        "            input_characters.add(char_item)\n",
        "        for char_item in target_text:\n",
        "            target_characters.add(char_item)"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OutOaTNo0d7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_characters = sorted(input_characters)\n",
        "target_characters = sorted(target_characters)\n",
        "\n",
        "encoder_token_num = len(input_characters)\n",
        "target_token_num = len(target_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(item) for item in input_texts])\n",
        "max_decoder_seq_length = max([len(item) for item in target_texts])\n",
        "\n",
        "encoder_token_index_dict = dict([(char_item, i) for i, char_item in enumerate(input_characters)])\n",
        "target_token_index_dict = dict([(char_item, i) for i, char_item in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, encoder_token_num), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, target_token_num), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, target_token_num), dtype='float32')"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5Y0rJKos0d7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,(input_text, target_text) in enumerate(zip(input_texts,target_texts)):\n",
        "    for j, char_item in enumerate(input_text):\n",
        "        encoder_input_data[i, j, encoder_token_index_dict[char_item]] = 1.\n",
        "    encoder_input_data[i, j+1:, encoder_token_index_dict[\" \"]] = 1.\n",
        "    for j, char_item in enumerate(target_text):\n",
        "        decoder_input_data[i, j, target_token_index_dict[char_item]] = 1.\n",
        "        if j > 0:\n",
        "            decoder_target_data[i , j-1, target_token_index_dict[char_item]] = 1.\n",
        "    decoder_input_data[i, j+1: ,target_token_index_dict[\" \"]] = 1.\n",
        "    decoder_target_data[i, j:, target_token_index_dict[\" \"]] = 1."
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQgkj_EO0d7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the Encoder\n",
        "encoder_inputs = Input(shape = (None, encoder_token_num))\n",
        "encoder_lstm = LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the Decoder\n",
        "decoder_inputs = Input(shape = (None, target_token_num))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(target_token_num, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/50\n32/32 [==============================] - 23s 721ms/step - loss: 2.3319 - accuracy: 0.6633 - val_loss: 2.6010 - val_accuracy: 0.5753\nEpoch 2/50\n32/32 [==============================] - 21s 658ms/step - loss: 1.8088 - accuracy: 0.6896 - val_loss: 2.8063 - val_accuracy: 0.5753\nEpoch 3/50\n32/32 [==============================] - 22s 681ms/step - loss: 1.7642 - accuracy: 0.6904 - val_loss: 2.4876 - val_accuracy: 0.5753\nEpoch 4/50\n32/32 [==============================] - 21s 665ms/step - loss: 1.7263 - accuracy: 0.6908 - val_loss: 2.5807 - val_accuracy: 0.5753\nEpoch 5/50\n32/32 [==============================] - 20s 614ms/step - loss: 1.6684 - accuracy: 0.6982 - val_loss: 2.6286 - val_accuracy: 0.5782\nEpoch 6/50\n32/32 [==============================] - 20s 610ms/step - loss: 1.5959 - accuracy: 0.7193 - val_loss: 2.5011 - val_accuracy: 0.6016\nEpoch 7/50\n32/32 [==============================] - 19s 600ms/step - loss: 1.5332 - accuracy: 0.7338 - val_loss: 2.2900 - val_accuracy: 0.6118\nEpoch 8/50\n32/32 [==============================] - 19s 586ms/step - loss: 1.4684 - accuracy: 0.7439 - val_loss: 2.3505 - val_accuracy: 0.6219\nEpoch 9/50\n32/32 [==============================] - 19s 585ms/step - loss: 1.4185 - accuracy: 0.7494 - val_loss: 2.1357 - val_accuracy: 0.6301\nEpoch 10/50\n32/32 [==============================] - 19s 580ms/step - loss: 1.3836 - accuracy: 0.7530 - val_loss: 2.0918 - val_accuracy: 0.6376\nEpoch 11/50\n32/32 [==============================] - 19s 583ms/step - loss: 1.3217 - accuracy: 0.7576 - val_loss: 2.0173 - val_accuracy: 0.6463\nEpoch 12/50\n32/32 [==============================] - 19s 580ms/step - loss: 1.3014 - accuracy: 0.7609 - val_loss: 1.9958 - val_accuracy: 0.6474\nEpoch 13/50\n32/32 [==============================] - 19s 583ms/step - loss: 1.2773 - accuracy: 0.7640 - val_loss: 1.9669 - val_accuracy: 0.6539\nEpoch 14/50\n32/32 [==============================] - 22s 688ms/step - loss: 1.2306 - accuracy: 0.7697 - val_loss: 1.9123 - val_accuracy: 0.6614\nEpoch 15/50\n32/32 [==============================] - 19s 580ms/step - loss: 1.2077 - accuracy: 0.7725 - val_loss: 2.0612 - val_accuracy: 0.6582\nEpoch 16/50\n32/32 [==============================] - 20s 627ms/step - loss: 1.1908 - accuracy: 0.7750 - val_loss: 1.8638 - val_accuracy: 0.6639\nEpoch 17/50\n32/32 [==============================] - 19s 591ms/step - loss: 1.1604 - accuracy: 0.7780 - val_loss: 1.8548 - val_accuracy: 0.6668\nEpoch 18/50\n32/32 [==============================] - 20s 622ms/step - loss: 1.1742 - accuracy: 0.7786 - val_loss: 1.8578 - val_accuracy: 0.6646\nEpoch 19/50\n32/32 [==============================] - 19s 587ms/step - loss: 1.1220 - accuracy: 0.7834 - val_loss: 1.8575 - val_accuracy: 0.6674\nEpoch 20/50\n32/32 [==============================] - 18s 574ms/step - loss: 1.1051 - accuracy: 0.7854 - val_loss: 1.8350 - val_accuracy: 0.6708\nEpoch 21/50\n32/32 [==============================] - 17s 547ms/step - loss: 1.0877 - accuracy: 0.7875 - val_loss: 1.7764 - val_accuracy: 0.6759\nEpoch 22/50\n32/32 [==============================] - 18s 556ms/step - loss: 1.0676 - accuracy: 0.7906 - val_loss: 1.7596 - val_accuracy: 0.6762\nEpoch 23/50\n32/32 [==============================] - 19s 588ms/step - loss: 1.0522 - accuracy: 0.7928 - val_loss: 1.7857 - val_accuracy: 0.6771\nEpoch 24/50\n32/32 [==============================] - 18s 573ms/step - loss: 1.0313 - accuracy: 0.7958 - val_loss: 1.7296 - val_accuracy: 0.6836\nEpoch 25/50\n32/32 [==============================] - 18s 563ms/step - loss: 1.0117 - accuracy: 0.7997 - val_loss: 1.7557 - val_accuracy: 0.6809\nEpoch 26/50\n32/32 [==============================] - 18s 548ms/step - loss: 0.9974 - accuracy: 0.8010 - val_loss: 1.7078 - val_accuracy: 0.6857\nEpoch 27/50\n32/32 [==============================] - 18s 550ms/step - loss: 0.9807 - accuracy: 0.8026 - val_loss: 1.7066 - val_accuracy: 0.6797\nEpoch 28/50\n32/32 [==============================] - 17s 539ms/step - loss: 0.9636 - accuracy: 0.8055 - val_loss: 1.8264 - val_accuracy: 0.6746\nEpoch 29/50\n32/32 [==============================] - 21s 645ms/step - loss: 0.9497 - accuracy: 0.8074 - val_loss: 1.6784 - val_accuracy: 0.6891\nEpoch 30/50\n32/32 [==============================] - 24s 738ms/step - loss: 0.9356 - accuracy: 0.8099 - val_loss: 1.7417 - val_accuracy: 0.6812\nEpoch 31/50\n32/32 [==============================] - 27s 838ms/step - loss: 0.9207 - accuracy: 0.8126 - val_loss: 1.7102 - val_accuracy: 0.6879\nEpoch 32/50\n32/32 [==============================] - 24s 737ms/step - loss: 0.9043 - accuracy: 0.8148 - val_loss: 1.6701 - val_accuracy: 0.6895\nEpoch 33/50\n32/32 [==============================] - 23s 731ms/step - loss: 0.8913 - accuracy: 0.8158 - val_loss: 1.6732 - val_accuracy: 0.6866\nEpoch 34/50\n32/32 [==============================] - 26s 807ms/step - loss: 0.8766 - accuracy: 0.8188 - val_loss: 1.6809 - val_accuracy: 0.6918\nEpoch 35/50\n32/32 [==============================] - 25s 767ms/step - loss: 0.8627 - accuracy: 0.8209 - val_loss: 1.6640 - val_accuracy: 0.6926\nEpoch 36/50\n32/32 [==============================] - 21s 655ms/step - loss: 0.8503 - accuracy: 0.8231 - val_loss: 1.6434 - val_accuracy: 0.6977\nEpoch 37/50\n32/32 [==============================] - 23s 725ms/step - loss: 0.8362 - accuracy: 0.8257 - val_loss: 1.6757 - val_accuracy: 0.6869\nEpoch 38/50\n32/32 [==============================] - 21s 665ms/step - loss: 0.8213 - accuracy: 0.8271 - val_loss: 1.6214 - val_accuracy: 0.6999\nEpoch 39/50\n32/32 [==============================] - 21s 655ms/step - loss: 0.8081 - accuracy: 0.8298 - val_loss: 1.6645 - val_accuracy: 0.7003\nEpoch 40/50\n32/32 [==============================] - 21s 653ms/step - loss: 0.7949 - accuracy: 0.8322 - val_loss: 1.6543 - val_accuracy: 0.6992\nEpoch 41/50\n32/32 [==============================] - 21s 646ms/step - loss: 0.7813 - accuracy: 0.8344 - val_loss: 1.6215 - val_accuracy: 0.6990\nEpoch 42/50\n32/32 [==============================] - 22s 680ms/step - loss: 0.7695 - accuracy: 0.8360 - val_loss: 1.6477 - val_accuracy: 0.6960\nEpoch 43/50\n32/32 [==============================] - 21s 659ms/step - loss: 0.7559 - accuracy: 0.8390 - val_loss: 1.6269 - val_accuracy: 0.6978\nEpoch 44/50\n32/32 [==============================] - 23s 708ms/step - loss: 0.7422 - accuracy: 0.8404 - val_loss: 1.6477 - val_accuracy: 0.7003\nEpoch 45/50\n32/32 [==============================] - 21s 661ms/step - loss: 0.7314 - accuracy: 0.8423 - val_loss: 1.6356 - val_accuracy: 0.6975\nEpoch 46/50\n32/32 [==============================] - 20s 639ms/step - loss: 0.7185 - accuracy: 0.8449 - val_loss: 1.6822 - val_accuracy: 0.6947\nEpoch 47/50\n32/32 [==============================] - 22s 690ms/step - loss: 0.7058 - accuracy: 0.8461 - val_loss: 1.6312 - val_accuracy: 0.7007\nEpoch 48/50\n32/32 [==============================] - 22s 698ms/step - loss: 0.6924 - accuracy: 0.8489 - val_loss: 1.6173 - val_accuracy: 0.7030\nEpoch 49/50\n32/32 [==============================] - 22s 686ms/step - loss: 0.6814 - accuracy: 0.8504 - val_loss: 1.6581 - val_accuracy: 0.6948\nEpoch 50/50\n32/32 [==============================] - 22s 682ms/step - loss: 0.6683 - accuracy: 0.8529 - val_loss: 1.6756 - val_accuracy: 0.6989\n"
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "model.save('s2s.h5')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "reverse_input_char_index_dict = dict(\n",
        "    (i, char) for char, i in encoder_token_index_dict.items())\n",
        "reverse_target_char_index_dict = dict(\n",
        "    (i, char) for char, i in target_token_index_dict.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_seq(input_seq):\n",
        "    states_value =  encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, target_token_num))\n",
        "    target_seq[0,0,target_token_index_dict['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index_dict[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if(sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1,target_token_num))\n",
        "        target_seq[0,0,sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h,c]\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Somebody was talking to Tom.\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n그 사람은 아직 도도 했어.\n\n"
        }
      ],
      "source": [
        "print(input_texts[2000])\n",
        "print(encoder_input_data[2000][3])\n",
        "print(decode_seq(encoder_input_data[2000:2001]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0\nInput: Go.\noutput: 그만해.\n\n1\nInput: Hi.\noutput: 이거 가져.\n\n2\nInput: Run!\noutput: 빨리!\n\n3\nInput: Run.\noutput: 그만해.\n\n4\nInput: Who?\noutput: 이거 가져.\n\n5\nInput: Wow!\noutput: 그만해!\n\n6\nInput: Fire!\noutput: 빨리!\n\n7\nInput: Help!\noutput: 빨리!\n\n8\nInput: Jump!\noutput: 빨리!\n\n9\nInput: Jump.\noutput: 그만해.\n\n10\nInput: Wait!\noutput: 그만 !\n\n11\nInput: Wait!\noutput: 그만 !\n\n12\nInput: Wait.\noutput: 그만해.\n\n13\nInput: Begin.\noutput: 그만해.\n\n14\nInput: Hello!\noutput: 빨리!\n\n15\nInput: I see.\noutput: 이건 내가 수가 있어.\n\n16\nInput: I try.\noutput: 이건 내가 수가 있어.\n\n17\nInput: I won!\noutput: 난 톰이 아직도 말해.\n\n18\nInput: Oh no!\noutput: 그만해.\n\n19\nInput: Relax.\noutput: 그만해.\n\n20\nInput: Shoot!\noutput: 빨리!\n\n21\nInput: Smile.\noutput: 그만 사람들이 아.\n\n22\nInput: Attack!\noutput: 빨리!\n\n23\nInput: Attack!\noutput: 빨리!\n\n24\nInput: Freeze!\noutput: 빨리!\n\n25\nInput: Get up.\noutput: 그만해.\n\n26\nInput: Got it!\noutput: 빨리!\n\n27\nInput: Hug me.\noutput: 그만해.\n\n28\nInput: I know.\noutput: 난 톰이 아직도 말 했어.\n\n29\nInput: I work.\noutput: 난 톰을 아직도 말 했어.\n\n30\nInput: Listen.\noutput: 그만 사람들은 아주 줘.\n\n31\nInput: No way!\noutput: 그만 !\n\n32\nInput: No way!\noutput: 그만 !\n\n33\nInput: Thanks.\noutput: 이건 내가 말이야.\n\n34\nInput: We try.\noutput: 우리가 와.\n\n35\nInput: We won.\noutput: 이거 가져.\n\n36\nInput: Why me?\noutput: 이거 가져.\n\n37\nInput: Awesome!\noutput: 빨리!\n\n38\nInput: Be fair.\noutput: 그만 사람들이 아.\n\n39\nInput: Beat it.\noutput: 그만해.\n\n40\nInput: Call us.\noutput: 그만 사람들은 아주 줘.\n\n41\nInput: Come in.\noutput: 그만 사람들이 아.\n\n42\nInput: Come on!\noutput: 빨리!\n\n43\nInput: Get out.\noutput: 그만 사람들이 아.\n\n44\nInput: Go away!\noutput: 빨리!\n\n45\nInput: Go away.\noutput: 그만 사람들이 아.\n\n46\nInput: Goodbye!\noutput: 그만 말해!\n\n47\nInput: He came.\noutput: 이거 가져.\n\n48\nInput: He came.\noutput: 이거 가져.\n\n49\nInput: Help me!\noutput: 빨리!\n\n50\nInput: Help me.\noutput: 그만 사람들이 아.\n\n51\nInput: Hit Tom.\noutput: 톰은 자기 있어.\n\n52\nInput: I agree.\noutput: 난 톰이 아직도 말 했어.\n\n53\nInput: I'm sad.\noutput: 난 톰이 아직도 안 했어.\n\n54\nInput: Me, too.\noutput: 그 사람은 아직 도도 했어.\n\n55\nInput: Open up.\noutput: 그만 사람들은 아주 줘.\n\n56\nInput: Perfect!\noutput: 빨리!\n\n57\nInput: Show me.\noutput: 이거 가져.\n\n58\nInput: Shut up!\noutput: 빨리!\n\n59\nInput: Skip it.\noutput: 그만 사람들이 아.\n\n60\nInput: Stop it.\noutput: 그만 사람들이 아.\n\n61\nInput: Tell me.\noutput: 톰은 아직도 도 해.\n\n62\nInput: Tom won.\noutput: 톰은 아직도 말해.\n\n63\nInput: Wake up!\noutput: 빨리!\n\n64\nInput: Wash up.\noutput: 우리가 와.\n\n65\nInput: Welcome.\noutput: 그만 사람들은 아주 줘.\n\n66\nInput: Welcome.\noutput: 그만 사람들은 아주 줘.\n\n67\nInput: Who won?\noutput: 이거 가져.\n\n68\nInput: Why not?\noutput: 이거 가져.\n\n69\nInput: Cheer up!\noutput: 빨리!\n\n70\nInput: Cool off!\noutput: 빨리리가!\n\n71\nInput: Get lost.\noutput: 그만 사람들이 아.\n\n72\nInput: Go ahead.\noutput: 그만 사람들은 아주 줘.\n\n73\nInput: Good job!\noutput: 빨리!\n\n74\nInput: Grab Tom.\noutput: 톰은 아직도 말해 줘.\n\n75\nInput: How cute!\noutput: 그만 !\n\n76\nInput: How cute!\noutput: 그만 !\n\n77\nInput: How deep?\noutput: 이거 가져?\n\n78\nInput: Hurry up.\noutput: 그만 사람들은 아주 줘.\n\n79\nInput: I forgot.\noutput: 난 톰이 아직도 안 했어.\n\n80\nInput: I'm ugly.\noutput: 난 톰이 거짓말 하지 않았어.\n\n81\nInput: It hurts.\noutput: 그건 내가 일이야.\n\n82\nInput: It works.\noutput: 이건 내가 수 있어.\n\n83\nInput: It works.\noutput: 이건 내가 수 있어.\n\n84\nInput: It works.\noutput: 이건 내가 수 있어.\n\n85\nInput: Let's go!\noutput: 빨리리가!\n\n86\nInput: Look out!\noutput: 빨리!\n\n87\nInput: Sit down!\noutput: 빨리!\n\n88\nInput: Sit here.\noutput: 우리가 와.\n\n89\nInput: Speak up!\noutput: 빨리!\n\n90\nInput: Stand up!\noutput: 빨리!\n\n91\nInput: Tell Tom.\noutput: 톰은 아직도 말해.\n\n92\nInput: Tell Tom.\noutput: 톰은 아직도 말해.\n\n93\nInput: They won.\noutput: 이건 내가 말이 없어.\n\n94\nInput: They won.\noutput: 이건 내가 말이 없어.\n\n95\nInput: Tom died.\noutput: 톰은 아직도 말해.\n\n96\nInput: Tom left.\noutput: 톰은 아직도 말해.\n\n97\nInput: Tom left.\noutput: 톰은 아직도 말해.\n\n98\nInput: Tom lied.\noutput: 톰은 아직도 말해.\n\n99\nInput: Tom lied.\noutput: 톰은 아직도 말해.\n\n100\nInput: Tom lost.\noutput: 톰은 아직도 말해.\n\n101\nInput: Tom paid.\noutput: 톰은 아직도 말해.\n\n102\nInput: Tom quit.\noutput: 톰은 아직도 말해.\n\n103\nInput: Tom wept.\noutput: 톰은 아직도 말해.\n\n104\nInput: Too late.\noutput: 톰은 아직도 말해.\n\n105\nInput: Trust me.\noutput: 이건 내가 말이야.\n\n106\nInput: Try hard.\noutput: 이건 내가 말 없어.\n\n107\nInput: Try some.\noutput: 이건 내가 말하는 것 같아.\n\n108\nInput: Try this.\noutput: 이건 내가 말이 없어.\n\n109\nInput: What for?\noutput: 이거 가져.\n\n110\nInput: What fun!\noutput: 그만 !\n\n111\nInput: What fun!\noutput: 그만 !\n\n112\nInput: Who died?\noutput: 이거 가져?\n\n113\nInput: Who quit?\noutput: 이거 가져.\n\n114\nInput: Answer me.\noutput: 이건 가져.\n\n115\nInput: Birds fly.\noutput: 그만 사람들은 아주 줘.\n\n116\nInput: Calm down.\noutput: 그만 사람들은 아주 줘.\n\n117\nInput: Come here.\noutput: 우리가 와.\n\n118\nInput: Come home.\noutput: 그만 사람들은 아주 줘.\n\n119\nInput: Dogs bark.\noutput: 그만 사람들은 아주 줘.\n\n120\nInput: Don't lie.\noutput: 그만 사람들은 아주 줘.\n\n121\nInput: Don't lie.\noutput: 그만 사람들은 아주 줘.\n\n122\nInput: Fantastic!\noutput: 말하지마!\n\n123\nInput: Follow me.\noutput: 그만 사람들이 아.\n\n124\nInput: Forget it.\noutput: 그만 사람들이 아.\n\n125\nInput: Forget me.\noutput: 그만 사람들이 아.\n\n126\nInput: Forget me.\noutput: 그만 사람들이 아.\n\n127\nInput: Get ready.\noutput: 그만 사람들은 아직도 줘.\n\n128\nInput: Good luck.\noutput: 그만 사람들은 아주 줘.\n\n129\nInput: Good luck.\noutput: 그만 사람들은 아주 줘.\n\n130\nInput: Grab this.\noutput: 그만 사람들은 아주 줘.\n\n131\nInput: Hands off.\noutput: 그 사람들은 어디어.\n\n132\nInput: He smiled.\noutput: 그 사람은 아직 도도 했어.\n\n133\nInput: Hold this.\noutput: 이거 가져.\n\n134\nInput: How awful!\noutput: 이렇게 귀엽다니!\n\n135\nInput: How awful!\noutput: 이렇게 귀엽다니!\n\n136\nInput: I fainted.\noutput: 난 톰이 거짓말 하지 않았어.\n\n137\nInput: I laughed.\noutput: 난 톰이 아직도 말 했어.\n\n138\nInput: I promise.\noutput: 난 톰을 아직도 말 했어.\n\n139\nInput: I'm sorry.\noutput: 난 톰이 아직도 안 했어.\n\n140\nInput: I'm sorry.\noutput: 난 톰이 아직도 안 했어.\n\n141\nInput: I'm sorry.\noutput: 난 톰이 아직도 안 했어.\n\n142\nInput: I'm sorry.\noutput: 난 톰이 아직도 안 했어.\n\n143\nInput: It rained.\noutput: 그건 내가 일이야.\n\n144\nInput: It rained.\noutput: 그건 내가 일이야.\n\n145\nInput: It snowed.\noutput: 이건 내가 수 있어.\n\n146\nInput: It stinks.\noutput: 이건 내가 수 있어.\n\n147\nInput: It's 7:45.\noutput: 이건 내가 수 있어.\n\n148\nInput: Kill them.\noutput: 그만 사람들은 아직도 줘.\n\n149\nInput: Leave now.\noutput: 그만 사람들은 아주 줘.\n\n150\nInput: Of course!\noutput: 빨리!\n\n151\nInput: Of course.\noutput: 이거 가져.\n\n152\nInput: Oh please!\noutput: 이렇게 미미해!\n\n153\nInput: Read this.\noutput: 그만 사람들은 아주 줘.\n\n154\nInput: Say hello.\noutput: 그만 사람들은 아주 줘.\n\n155\nInput: See below.\noutput: 그 사람은 아직 시도 했어.\n\n156\nInput: Seriously?\noutput: 그만 사람들은 아주 줘.\n\n157\nInput: Sit there.\noutput: 그만 사람들은 아주 줘.\n\n158\nInput: Sit tight.\noutput: 그만 사람들이 아.\n\n159\nInput: Start now.\noutput: 그만 사람들이 아.\n\n160\nInput: Stay calm.\noutput: 그만 사람들이 아.\n\n161\nInput: Stay here.\noutput: 그만 사람들은 아주 줘.\n\n162\nInput: Step back.\noutput: 그만 사람들이 아.\n\n163\nInput: Stop here.\noutput: 그만 사람들은 아주 줘.\n\n164\nInput: Take care.\noutput: 이건 내가 말이야.\n\n165\nInput: Take this.\noutput: 이건 내가 말이야.\n\n166\nInput: Take this.\noutput: 이건 내가 말이야.\n\n167\nInput: Take this.\noutput: 이건 내가 말이야.\n\n168\nInput: Thank you.\noutput: 이건 내가 말이야.\n\n169\nInput: Then what?\noutput: 이건 내가 말이야.\n\n170\nInput: They left.\noutput: 이건 내가 말 없어.\n\n171\nInput: They left.\noutput: 이건 내가 말 없어.\n\n172\nInput: They left.\noutput: 이건 내가 말 없어.\n\n173\nInput: They lied.\noutput: 이건 내가 말 없어.\n\n174\nInput: They lost.\noutput: 이건 내가 말이 없어.\n\n175\nInput: They lost.\noutput: 이건 내가 말이 없어.\n\n176\nInput: Tom cried.\noutput: 톰은 아직도 말해.\n\n177\nInput: Tom dozed.\noutput: 톰은 아직도 말해.\n\n178\nInput: Tom drove.\noutput: 톰은 아직도 말해.\n\n179\nInput: Tom knits.\noutput: 톰은 아직도 말해.\n\n180\nInput: Tom knows.\noutput: 톰은 아직도 말해.\n\n181\nInput: Try again.\noutput: 이건 내가 말이 없어.\n\n182\nInput: Turn left.\noutput: 이건 내가 말이야.\n\n183\nInput: Turn left.\noutput: 이건 내가 말이야.\n\n184\nInput: Wait here.\noutput: 우리가 와.\n\n185\nInput: Watch out!\noutput: 빨리!\n\n186\nInput: We talked.\noutput: 우리가 와.\n\n187\nInput: We waited.\noutput: 이거 가져.\n\n188\nInput: Well done!\noutput: 그만 가!\n\n189\nInput: Who cares?\noutput: 이건 어디가 있어?\n\n190\nInput: Who knows?\noutput: 이거 가져.\n\n191\nInput: Wonderful!\noutput: 빨리!\n\n192\nInput: You idiot!\noutput: 그는 사람은 아직 시간을 필요해.\n\n193\nInput: You tried.\noutput: 그 사람은 아직 도 했어.\n\n194\nInput: All aboard!\noutput: 그만 말해!\n\n195\nInput: Ask anyone.\noutput: 이거 사과해.\n\n196\nInput: Be careful.\noutput: 그만 사람들은 아직도 줘.\n\n197\nInput: Be patient.\noutput: 그만 사람들이 아.\n\n198\nInput: Be patient.\noutput: 그만 사람들이 아.\n\n199\nInput: Birds sing.\noutput: 이거 가져.\n\n"
        }
      ],
      "source": [
        "for idx in range(200):\n",
        "    print(idx)\n",
        "    input_seq = encoder_input_data[idx: idx+1]\n",
        "    decoded_sent = decode_seq(input_seq)\n",
        "    print('Input:', input_texts[idx])\n",
        "    print('output:', decoded_sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}