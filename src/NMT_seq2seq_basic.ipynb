{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5hhSZna0zh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "849b4e4a-6f47-47a6-8179-6cecb552b2ae"
      },
      "source": [
        "#For Colab ENV\n",
        "from google.colab import drive \n",
        "drive.mount('/gdrive')\n",
        "!git clone https://github.com/FacerAin/Kor_Eng_NMT_System.git '/gdrive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "Cloning into '/gdrive/My Drive/Colab Notebooks'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 18 (delta 2), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dpGBb-X0d68",
        "colab_type": "text"
      },
      "source": [
        "## NMT_seq2seq_basic\n",
        "\n",
        "Character Level NMT based on Simple Seq2Seq Model Without Attention Mechanism \n",
        "\n",
        "Refer to https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V27ojmrJ0d6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIyZgSr0d7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameter Setting\n",
        "dataset_dir_path = '../dataset/kor-eng/'\n",
        "batch_size = 64\n",
        "latent_dim = 256\n",
        "epochs = 100\n",
        "num_samples = 3500"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfBWDMn0d7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "d7BvUjss0d7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read Dataset and Preprocessing\n",
        "with open(dataset_dir_path+\"kor.txt\", \"r\", encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for line in lines[:min(num_samples, len(lines)-2)]:#To Except Last Line\n",
        "        split_line = line.split('\\t')\n",
        "        input_text = split_line[0]\n",
        "        target_text = '\\t' + split_line[1]+ '\\n' #Using '^' to start tag, '&' to end tag\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for char_item in input_text:\n",
        "            input_characters.add(char_item)\n",
        "        for char_item in target_text:\n",
        "            target_characters.add(char_item)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OutOaTNo0d7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_characters = sorted(input_characters)\n",
        "target_characters = sorted(target_characters)\n",
        "\n",
        "encoder_token_num = len(input_characters)\n",
        "target_token_num = len(target_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(item) for item in input_texts])\n",
        "max_decoder_seq_length = max([len(item) for item in target_texts])\n",
        "\n",
        "encoder_token_index_dict = dict([(char_item, i) for i, char_item in enumerate(input_characters)])\n",
        "target_token_index_dict = dict([(char_item, i) for i, char_item in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, encoder_token_num), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, target_token_num), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, target_token_num), dtype='float32')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5Y0rJKos0d7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,(input_text, target_text) in enumerate(zip(input_texts,target_texts)):\n",
        "    for j, char_item in enumerate(input_text):\n",
        "        encoder_input_data[i, j, encoder_token_index_dict[char_item]] = 1.\n",
        "    encoder_input_data[i, j+1:, encoder_token_index_dict[\" \"]] = 1.\n",
        "    for j, char_item in enumerate(target_text):\n",
        "        decoder_input_data[i, j, target_token_index_dict[char_item]] = 1.\n",
        "        if j > 0:\n",
        "            decoder_target_data[i , j-1, target_token_index_dict[char_item]] = 1.\n",
        "    decoder_input_data[i, j+1: ,target_token_index_dict[\" \"]] = 1.\n",
        "    decoder_target_data[i, j:, target_token_index_dict[\" \"]] = 1."
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQgkj_EO0d7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the Encoder\n",
        "encoder_inputs = Input(shape = (None, encoder_token_num))\n",
        "encoder_lstm = LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the Decoder\n",
        "decoder_inputs = Input(shape = (None, target_token_num))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(target_token_num, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/100\n44/44 [==============================] - 10s 237ms/step - loss: 1.9485 - accuracy: 0.7230 - val_loss: 2.5687 - val_accuracy: 0.6085\nEpoch 2/100\n44/44 [==============================] - 10s 224ms/step - loss: 1.5231 - accuracy: 0.7433 - val_loss: 2.4019 - val_accuracy: 0.6085\nEpoch 3/100\n44/44 [==============================] - 10s 223ms/step - loss: 1.4748 - accuracy: 0.7437 - val_loss: 2.4845 - val_accuracy: 0.6085\nEpoch 4/100\n44/44 [==============================] - 10s 226ms/step - loss: 1.4095 - accuracy: 0.7534 - val_loss: 2.2983 - val_accuracy: 0.6264\nEpoch 5/100\n44/44 [==============================] - 10s 219ms/step - loss: 1.3318 - accuracy: 0.7718 - val_loss: 2.2162 - val_accuracy: 0.6340\nEpoch 6/100\n44/44 [==============================] - 10s 221ms/step - loss: 1.2592 - accuracy: 0.7826 - val_loss: 2.9498 - val_accuracy: 0.6129\nEpoch 7/100\n44/44 [==============================] - 10s 220ms/step - loss: 1.2139 - accuracy: 0.7873 - val_loss: 2.0681 - val_accuracy: 0.6435\nEpoch 8/100\n44/44 [==============================] - 10s 221ms/step - loss: 1.1475 - accuracy: 0.7929 - val_loss: 1.9752 - val_accuracy: 0.6536\nEpoch 9/100\n44/44 [==============================] - 10s 221ms/step - loss: 1.1193 - accuracy: 0.7963 - val_loss: 1.9107 - val_accuracy: 0.6670\nEpoch 10/100\n44/44 [==============================] - 10s 222ms/step - loss: 1.0779 - accuracy: 0.8007 - val_loss: 1.8628 - val_accuracy: 0.6717\nEpoch 11/100\n44/44 [==============================] - 10s 222ms/step - loss: 1.0565 - accuracy: 0.8045 - val_loss: 1.8376 - val_accuracy: 0.6754\nEpoch 12/100\n44/44 [==============================] - 10s 218ms/step - loss: 1.0170 - accuracy: 0.8088 - val_loss: 1.8151 - val_accuracy: 0.6772\nEpoch 13/100\n44/44 [==============================] - 10s 221ms/step - loss: 1.0020 - accuracy: 0.8115 - val_loss: 1.7695 - val_accuracy: 0.6807\nEpoch 14/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.9687 - accuracy: 0.8154 - val_loss: 1.7273 - val_accuracy: 0.6853\nEpoch 15/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.9487 - accuracy: 0.8184 - val_loss: 1.7719 - val_accuracy: 0.6824\nEpoch 16/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.9284 - accuracy: 0.8206 - val_loss: 1.6761 - val_accuracy: 0.6919\nEpoch 17/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.9055 - accuracy: 0.8238 - val_loss: 1.6741 - val_accuracy: 0.6947\nEpoch 18/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.8863 - accuracy: 0.8266 - val_loss: 1.6687 - val_accuracy: 0.6953\nEpoch 19/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.8692 - accuracy: 0.8285 - val_loss: 1.6397 - val_accuracy: 0.6989\nEpoch 20/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.8514 - accuracy: 0.8306 - val_loss: 1.6422 - val_accuracy: 0.6977\nEpoch 21/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.8351 - accuracy: 0.8330 - val_loss: 1.6042 - val_accuracy: 0.7007\nEpoch 22/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.8228 - accuracy: 0.8343 - val_loss: 1.5927 - val_accuracy: 0.7031\nEpoch 23/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.8099 - accuracy: 0.8363 - val_loss: 1.5857 - val_accuracy: 0.7062\nEpoch 24/100\n44/44 [==============================] - 10s 218ms/step - loss: 0.7914 - accuracy: 0.8389 - val_loss: 1.5728 - val_accuracy: 0.7065\nEpoch 25/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.8297 - accuracy: 0.8359 - val_loss: 1.5592 - val_accuracy: 0.7050\nEpoch 26/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.7648 - accuracy: 0.8428 - val_loss: 1.5661 - val_accuracy: 0.7058\nEpoch 27/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.7514 - accuracy: 0.8451 - val_loss: 1.5477 - val_accuracy: 0.7098\nEpoch 28/100\n44/44 [==============================] - 10s 225ms/step - loss: 0.7398 - accuracy: 0.8460 - val_loss: 1.5593 - val_accuracy: 0.7040\nEpoch 29/100\n44/44 [==============================] - 10s 219ms/step - loss: 0.7277 - accuracy: 0.8480 - val_loss: 1.5725 - val_accuracy: 0.7080\nEpoch 30/100\n44/44 [==============================] - 10s 219ms/step - loss: 0.7144 - accuracy: 0.8499 - val_loss: 1.5357 - val_accuracy: 0.7129\nEpoch 31/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.7018 - accuracy: 0.8522 - val_loss: 1.5428 - val_accuracy: 0.7110\nEpoch 32/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.6925 - accuracy: 0.8531 - val_loss: 1.5449 - val_accuracy: 0.7121\nEpoch 33/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.6822 - accuracy: 0.8552 - val_loss: 1.5580 - val_accuracy: 0.7075\nEpoch 34/100\n44/44 [==============================] - 10s 224ms/step - loss: 0.6720 - accuracy: 0.8569 - val_loss: 1.5440 - val_accuracy: 0.7086\nEpoch 35/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.6601 - accuracy: 0.8584 - val_loss: 1.5491 - val_accuracy: 0.7111\nEpoch 36/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.6496 - accuracy: 0.8602 - val_loss: 1.5423 - val_accuracy: 0.7122\nEpoch 37/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.6540 - accuracy: 0.8601 - val_loss: 1.5374 - val_accuracy: 0.7139\nEpoch 38/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.6278 - accuracy: 0.8640 - val_loss: 1.5482 - val_accuracy: 0.7128\nEpoch 39/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.6203 - accuracy: 0.8655 - val_loss: 1.5561 - val_accuracy: 0.7119\nEpoch 40/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.6079 - accuracy: 0.8679 - val_loss: 1.5612 - val_accuracy: 0.7116\nEpoch 41/100\n44/44 [==============================] - 10s 225ms/step - loss: 0.5977 - accuracy: 0.8697 - val_loss: 1.5730 - val_accuracy: 0.7090\nEpoch 42/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.5890 - accuracy: 0.8718 - val_loss: 1.5398 - val_accuracy: 0.7114\nEpoch 43/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.5774 - accuracy: 0.8737 - val_loss: 1.5595 - val_accuracy: 0.7126\nEpoch 44/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.5666 - accuracy: 0.8750 - val_loss: 1.5610 - val_accuracy: 0.7114\nEpoch 45/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.5588 - accuracy: 0.8768 - val_loss: 1.5764 - val_accuracy: 0.7102\nEpoch 46/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.5478 - accuracy: 0.8793 - val_loss: 1.5720 - val_accuracy: 0.7117\nEpoch 47/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.5381 - accuracy: 0.8811 - val_loss: 1.5746 - val_accuracy: 0.7109\nEpoch 48/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.5271 - accuracy: 0.8831 - val_loss: 1.5771 - val_accuracy: 0.7108\nEpoch 49/100\n44/44 [==============================] - 9s 215ms/step - loss: 0.5186 - accuracy: 0.8845 - val_loss: 1.6103 - val_accuracy: 0.7076\nEpoch 50/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.5092 - accuracy: 0.8866 - val_loss: 1.6064 - val_accuracy: 0.7089\nEpoch 51/100\n44/44 [==============================] - 9s 215ms/step - loss: 0.4994 - accuracy: 0.8890 - val_loss: 1.6249 - val_accuracy: 0.7105\nEpoch 52/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.4921 - accuracy: 0.8901 - val_loss: 1.6387 - val_accuracy: 0.7036\nEpoch 53/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.4846 - accuracy: 0.8920 - val_loss: 1.6291 - val_accuracy: 0.7100\nEpoch 54/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.4730 - accuracy: 0.8942 - val_loss: 1.6437 - val_accuracy: 0.7081\nEpoch 55/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.4649 - accuracy: 0.8964 - val_loss: 1.6555 - val_accuracy: 0.7069\nEpoch 56/100\n44/44 [==============================] - 10s 225ms/step - loss: 0.4557 - accuracy: 0.8980 - val_loss: 1.6571 - val_accuracy: 0.7100\nEpoch 57/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.4467 - accuracy: 0.9003 - val_loss: 1.6505 - val_accuracy: 0.7070\nEpoch 58/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.4394 - accuracy: 0.9019 - val_loss: 1.6848 - val_accuracy: 0.7050\nEpoch 59/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.4298 - accuracy: 0.9038 - val_loss: 1.6909 - val_accuracy: 0.7054\nEpoch 60/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.4223 - accuracy: 0.9057 - val_loss: 1.6707 - val_accuracy: 0.7068\nEpoch 61/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.4149 - accuracy: 0.9071 - val_loss: 1.6905 - val_accuracy: 0.7059\nEpoch 62/100\n44/44 [==============================] - 10s 217ms/step - loss: 0.4075 - accuracy: 0.9093 - val_loss: 1.7002 - val_accuracy: 0.7064\nEpoch 63/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.3981 - accuracy: 0.9120 - val_loss: 1.7084 - val_accuracy: 0.7068\nEpoch 64/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.3900 - accuracy: 0.9136 - val_loss: 1.7480 - val_accuracy: 0.7022\nEpoch 65/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.3818 - accuracy: 0.9149 - val_loss: 1.7298 - val_accuracy: 0.7058\nEpoch 66/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.3754 - accuracy: 0.9164 - val_loss: 1.7324 - val_accuracy: 0.7043\nEpoch 67/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.3692 - accuracy: 0.9178 - val_loss: 1.7482 - val_accuracy: 0.7063\nEpoch 68/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.3655 - accuracy: 0.9192 - val_loss: 1.7295 - val_accuracy: 0.7085\nEpoch 69/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.3542 - accuracy: 0.9213 - val_loss: 1.7478 - val_accuracy: 0.7057\nEpoch 70/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.3467 - accuracy: 0.9236 - val_loss: 1.7753 - val_accuracy: 0.7021\nEpoch 71/100\n44/44 [==============================] - 10s 224ms/step - loss: 0.3421 - accuracy: 0.9237 - val_loss: 1.7615 - val_accuracy: 0.7047\nEpoch 72/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.3348 - accuracy: 0.9261 - val_loss: 1.7844 - val_accuracy: 0.7030\nEpoch 73/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.3300 - accuracy: 0.9269 - val_loss: 1.7833 - val_accuracy: 0.7046\nEpoch 74/100\n44/44 [==============================] - 10s 218ms/step - loss: 0.3224 - accuracy: 0.9287 - val_loss: 1.8090 - val_accuracy: 0.7045\nEpoch 75/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.3194 - accuracy: 0.9295 - val_loss: 1.8172 - val_accuracy: 0.7036\nEpoch 76/100\n44/44 [==============================] - 9s 216ms/step - loss: 0.3102 - accuracy: 0.9318 - val_loss: 1.8320 - val_accuracy: 0.7045\nEpoch 77/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.3080 - accuracy: 0.9319 - val_loss: 1.8474 - val_accuracy: 0.7049\nEpoch 78/100\n44/44 [==============================] - 10s 216ms/step - loss: 0.3012 - accuracy: 0.9339 - val_loss: 1.8475 - val_accuracy: 0.7030\nEpoch 79/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.2962 - accuracy: 0.9345 - val_loss: 1.8374 - val_accuracy: 0.7043\nEpoch 80/100\n44/44 [==============================] - 10s 224ms/step - loss: 0.2887 - accuracy: 0.9365 - val_loss: 1.8707 - val_accuracy: 0.7023\nEpoch 81/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.2849 - accuracy: 0.9376 - val_loss: 1.8695 - val_accuracy: 0.7035\nEpoch 82/100\n44/44 [==============================] - 10s 235ms/step - loss: 0.2805 - accuracy: 0.9385 - val_loss: 1.8779 - val_accuracy: 0.7014\nEpoch 83/100\n44/44 [==============================] - 10s 237ms/step - loss: 0.2727 - accuracy: 0.9402 - val_loss: 1.9029 - val_accuracy: 0.7002\nEpoch 84/100\n44/44 [==============================] - 10s 223ms/step - loss: 0.2701 - accuracy: 0.9403 - val_loss: 1.8841 - val_accuracy: 0.7029\nEpoch 85/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.2638 - accuracy: 0.9422 - val_loss: 1.8869 - val_accuracy: 0.7011\nEpoch 86/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.2597 - accuracy: 0.9429 - val_loss: 1.9265 - val_accuracy: 0.6999\nEpoch 87/100\n44/44 [==============================] - 10s 224ms/step - loss: 0.2553 - accuracy: 0.9439 - val_loss: 1.9212 - val_accuracy: 0.6977\nEpoch 88/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.2535 - accuracy: 0.9449 - val_loss: 1.9290 - val_accuracy: 0.7022\nEpoch 89/100\n44/44 [==============================] - 10s 226ms/step - loss: 0.2477 - accuracy: 0.9454 - val_loss: 1.9597 - val_accuracy: 0.7011\nEpoch 90/100\n44/44 [==============================] - 10s 224ms/step - loss: 0.2423 - accuracy: 0.9466 - val_loss: 1.9517 - val_accuracy: 0.7024\nEpoch 91/100\n44/44 [==============================] - 10s 219ms/step - loss: 0.2382 - accuracy: 0.9474 - val_loss: 1.9714 - val_accuracy: 0.6984\nEpoch 92/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.2333 - accuracy: 0.9486 - val_loss: 1.9759 - val_accuracy: 0.6983\nEpoch 93/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.2308 - accuracy: 0.9492 - val_loss: 1.9668 - val_accuracy: 0.7031\nEpoch 94/100\n44/44 [==============================] - 10s 222ms/step - loss: 0.2263 - accuracy: 0.9503 - val_loss: 1.9910 - val_accuracy: 0.6995\nEpoch 95/100\n44/44 [==============================] - 10s 224ms/step - loss: 0.2246 - accuracy: 0.9505 - val_loss: 1.9980 - val_accuracy: 0.7018\nEpoch 96/100\n44/44 [==============================] - 10s 217ms/step - loss: 0.2193 - accuracy: 0.9516 - val_loss: 1.9977 - val_accuracy: 0.7016\nEpoch 97/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.2179 - accuracy: 0.9517 - val_loss: 2.0139 - val_accuracy: 0.6975\nEpoch 98/100\n44/44 [==============================] - 10s 220ms/step - loss: 0.2133 - accuracy: 0.9527 - val_loss: 2.0213 - val_accuracy: 0.6998\nEpoch 99/100\n44/44 [==============================] - 10s 225ms/step - loss: 0.2106 - accuracy: 0.9535 - val_loss: 2.0387 - val_accuracy: 0.6983\nEpoch 100/100\n44/44 [==============================] - 10s 221ms/step - loss: 0.2080 - accuracy: 0.9534 - val_loss: 2.0271 - val_accuracy: 0.7014\n"
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "model.save('s2s.h5')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "reverse_input_char_index_dict = dict(\n",
        "    (i, char) for char, i in encoder_token_index_dict.items())\n",
        "reverse_target_char_index_dict = dict(\n",
        "    (i, char) for char, i in target_token_index_dict.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_seq(input_seq):\n",
        "    states_value =  encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, target_token_num))\n",
        "    target_seq[0,0,target_token_index_dict['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index_dict[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if(sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1,target_token_num))\n",
        "        target_seq[0,0,sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h,c]\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Somebody was talking to Tom.\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0.]\n톰이 얼마나 심하게 맞았니?\n\n"
        }
      ],
      "source": [
        "print(input_texts[2000])\n",
        "print(encoder_input_data[2000][3])\n",
        "print(decode_seq(encoder_input_data[2000:2001]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0\nInput: Go.\noutput: 가져.\n\n1\nInput: Hi.\noutput: 우리가 해.\n\n2\nInput: Run!\noutput: 도와!\n\n3\nInput: Run.\noutput: 저리 가.\n\n4\nInput: Who?\noutput: 누가 이겼어?\n\n5\nInput: Wow!\noutput: 도와줘!\n\n6\nInput: Fire!\noutput: 이렇게 귀엽다니!\n\n7\nInput: Help!\noutput: 도와!\n\n8\nInput: Jump!\noutput: 도와!\n\n9\nInput: Jump.\noutput: 가져.\n\n10\nInput: Wait!\noutput: 쏴!\n\n11\nInput: Wait!\noutput: 쏴!\n\n12\nInput: Wait.\noutput: 저리 가.\n\n13\nInput: Begin.\noutput: 저리 가.\n\n14\nInput: Hello!\noutput: 이렇게 귀엽다니!\n\n15\nInput: I see.\noutput: 그 사람들 들 거야.\n\n16\nInput: I try.\noutput: 우린 해.\n\n17\nInput: I won!\noutput: 그가 왔어.\n\n18\nInput: Oh no!\noutput: 어서 와!\n\n19\nInput: Relax.\noutput: 저리 가.\n\n20\nInput: Shoot!\noutput: 이렇게 귀엽다니!\n\n21\nInput: Smile.\noutput: 저리 가.\n\n22\nInput: Attack!\noutput: 이렇게 끔찍하다니!\n\n23\nInput: Attack!\noutput: 이렇게 끔찍하다니!\n\n24\nInput: Freeze!\noutput: 이렇게 귀엽다니!\n\n25\nInput: Get up.\noutput: 가져.\n\n26\nInput: Got it!\noutput: 이렇게 귀엽다니!\n\n27\nInput: Hug me.\noutput: 저리 가.\n\n28\nInput: I know.\noutput: 나도 내 가 없어.\n\n29\nInput: I work.\noutput: 난 아주 내 아.\n\n30\nInput: Listen.\noutput: 저리 가.\n\n31\nInput: No way!\noutput: 그 사람이 왔어.\n\n32\nInput: No way!\noutput: 그 사람이 왔어.\n\n33\nInput: Thanks.\noutput: 그거 사라.\n\n34\nInput: We try.\noutput: 이거 거짓말 하지 마.\n\n35\nInput: We won.\noutput: 누가 이겼어.\n\n36\nInput: Why me?\noutput: 누가 좋아해?\n\n37\nInput: Awesome!\noutput: 이렇게 끔찍하다니!\n\n38\nInput:Be fair.\noutput: 저걸 해.\n\n39\nInput: Beat it.\noutput: 저리 가.\n\n40\nInput: Call us.\noutput: 저리 가.\n\n41\nInput: Come in.\noutput: 저리 가.\n\n42\nInput: Come on!\noutput: 빨리 와!\n\n43\nInput: Get out.\noutput: 저리 가.\n\n44\nInput: Go away!\noutput: 빨리 와!\n\n45\nInput: Go away.\noutput: 저리 가.\n\n46\nInput: Goodbye!\noutput: 빨리 와!\n\n47\nInput: He came.\noutput: 그 사람이 왔어.\n\n48\nInput: He came.\noutput: 그 사람이 왔어.\n\n49\nInput: Help me!\noutput: 도와!\n\n50\nInput: Help me.\noutput: 도와줘.\n\n51\nInput: Hit Tom.\noutput: 저리 가.\n\n52\nInput: I agree.\noutput: 난 아주 내 아.\n\n53\nInput: I'm sad.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n54\nInput: Me, too.\noutput: 그 사람들은 거짓말 하지 않아.\n\n55\nInput: Open up.\noutput: 저걸 해.\n\n56\nInput: Perfect!\noutput: 이렇게 끔찍하다니!\n\n57\nInput: Show me.\noutput: 저리 가.\n\n58\nInput: Shut up!\noutput: 이렇게 귀엽다니!\n\n59\nInput: Skip it.\noutput: 저리 가.\n\n60\nInput: Stop it.\noutput: 저리 가.\n\n61\nInput: Tell me.\noutput: 그 사람들 .\n\n62\nInput: Tom won.\noutput: 톰이 거짓말 했어.\n\n63\nInput: Wake up!\noutput: 어서!\n\n64\nInput: Wash up.\noutput: 이거 가져.\n\n65\nInput: Welcome.\noutput: 저리 가.\n\n66\nInput: Welcome.\noutput: 저리 가.\n\n67\nInput: Who won?\noutput: 누가 좋아해?\n\n68\nInput: Why not?\noutput: 누가 좋아해?\n\n69\nInput: Cheer up!\noutput: 이렇게 귀엽다니!\n\n70\nInput: Cool off!\noutput: 이렇게 귀엽다니!\n\n71\nInput: Get lost.\noutput: 저리 가.\n\n72\nInput: Go ahead.\noutput: 저리 가.\n\n73\nInput: Good job!\noutput: 이렇게 끔찍하다니!\n\n74\nInput: Grab Tom.\noutput: 그만 떠나.\n\n75\nInput: How cute!\noutput: 이렇게 끔찍하다니!\n\n76\nInput: How cute!\noutput: 이렇게 끔찍하다니!\n\n77\nInput: How deep?\noutput: 누가 좋아해?\n\n78\nInput: Hurry up.\noutput: 이거 와.\n\n79\nInput: I forgot.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n80\nInput: I'm ugly.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n81\nInput: It hurts.\noutput: 그건 내 거야.\n\n82\nInput: It works.\noutput: 그건 내 가 가 있어.\n\n83\nInput: It works.\noutput: 그건 내 가 가 있어.\n\n84\nInput: It works.\noutput: 그건 내 가 가 있어.\n\n85\nInput: Let's go!\noutput: 빨리 와!\n\n86\nInput: Look out!\noutput: 이렇게 귀엽다니!\n\n87\nInput: Sit down!\noutput: 이렇게 귀엽다니!\n\n88\nInput: Sit here.\noutput: 이거 거짓말 하지 마.\n\n89\nInput: Speak up!\noutput: 이렇게 귀엽다니!\n\n90\nInput: Stand up!\noutput: 이렇게 귀엽다니!\n\n91\nInput: Tell Tom.\noutput: 톰이 웃었어.\n\n92\nInput: Tell Tom.\noutput: 톰이 웃었어.\n\n93\nInput: They won.\noutput: 그들이 사라졌어.\n\n94\nInput: They won.\noutput: 그들이 사라졌어.\n\n95\nInput: Tom died.\noutput: 톰이 거짓말 했어.\n\n96\nInput: Tom left.\noutput: 톰이 거짓말 했어.\n\n97\nInput: Tom left.\noutput: 톰이 거짓말 했어.\n\n98\nInput: Tom lied.\noutput: 톰이 거짓말 했어.\n\n99\nInput: Tom lied.\noutput: 톰이 거짓말 했어.\n\n100\nInput: Tom lost.\noutput: 톰이 거짓말 했어.\n\n101\nInput: Tom paid.\noutput: 톰이 거짓말 했어.\n\n102\nInput: Tom quit.\noutput: 톰이 거짓말 했어.\n\n103\nInput: Tom wept.\noutput: 톰이 거짓말 했어.\n\n104\nInput: Too late.\noutput: 톰이 거짓말 하지 않아.\n\n105\nInput: Trust me.\noutput: 이거 말해해.\n\n106\nInput: Try hard.\noutput: 이건 좋아해.\n\n107\nInput: Try some.\noutput: 누군가 웃었어.\n\n108\nInput: Try this.\noutput: 누군가 죽었어.\n\n109\nInput: What for?\noutput: 이 해?\n\n110\nInput: What fun!\noutput: 이렇게 귀엽다니!\n\n111\nInput: What fun!\noutput: 이렇게 귀엽다니!\n\n112\nInput: Who died?\noutput: 누가 좋아해?\n\n113\nInput: Who quit?\noutput: 누가 좋아해?\n\n114\nInput: Answer me.\noutput: 이거 좋아해.\n\n115\nInput: Birds fly.\noutput: 이걸 봐.\n\n116\nInput: Calm down.\noutput: 그만 떠나.\n\n117\nInput: Come here.\noutput: 저걸 가져.\n\n118\nInput: Come home.\noutput: 저걸 가져.\n\n119\nInput: Dogs bark.\noutput: 그만 떠나.\n\n120\nInput: Don't lie.\noutput: 이걸 봐.\n\n121\nInput: Don't lie.\noutput: 이걸 봐.\n\n122\nInput: Fantastic!\noutput: 이렇게 끔찍하다니!\n\n123\nInput: Follow me.\noutput: 그만 사라.\n\n124\nInput: Forget it.\noutput: 그만 떠나.\n\n125\nInput: Forget me.\noutput: 그만 떠나.\n\n126\nInput: Forget me.\noutput: 그만 떠나.\n\n127\nInput: Get ready.\noutput: 저리 가.\n\n128\nInput: Good luck.\noutput: 저리 가.\n\n129\nInput: Good luck.\noutput: 저리 가.\n\n130\nInput: Grab this.\noutput: 그만 떠나.\n\n131\nInput: Hands off.\noutput: 이걸 봐.\n\n132\nInput: He smiled.\noutput: 그는 그 사람들 사람이 아.\n\n133\nInput: Hold this.\noutput: 이걸 봐.\n\n134\nInput: How awful!\noutput: 이렇게 끔찍하다니!\n\n135\nInput: How awful!\noutput: 이렇게 끔찍하다니!\n\n136\nInput: I fainted.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n137\nInput: I laughed.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n138\nInput: I promise.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n139\nInput: I'm sorry.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n140\nInput: I'm sorry.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n141\nInput: I'm sorry.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n142\nInput: I'm sorry.\noutput: 난 톰이 거짓말 하는 것 같아.\n\n143\nInput: It rained.\noutput: 그건 내 거야.\n\n144\nInput: It rained.\noutput: 그건 내 거야.\n\n145\nInput: It snowed.\noutput: 그건 내 사람이야.\n\n146\nInput: It stinks.\noutput: 그건 내 사전이야.\n\n147\nInput: It's 7:45.\noutput: 그건 내 사람이 사람이야.\n\n148\nInput: Kill them.\noutput: 그만 떠나.\n\n149\nInput: Leave now.\noutput: 저걸 가져.\n\n150\nInput: Of course!\noutput: 그만 싸워!\n\n151\nInput: Of course.\noutput: 이걸 봐.\n\n152\nInput: Oh please!\noutput: 이렇게 끔찍하다니!\n\n153\nInput: Read this.\noutput: 이걸 봐.\n\n154\nInput: Say hello.\noutput: 이걸 봐.\n\n155\nInput: See below.\noutput: 우린 기로 시작해.\n\n156\nInput: Seriously?\noutput: 이 좀 봐.\n\n157\nInput: Sit there.\noutput: 이걸 봐.\n\n158\nInput: Sit tight.\noutput: 저리 가.\n\n159\nInput: Start now.\noutput: 그만 떠나.\n\n160\nInput: Stay calm.\noutput: 그만 떠나.\n\n161\nInput: Stay here.\noutput: 이걸 봐.\n\n162\nInput: Step back.\noutput: 그만 떠나.\n\n163\nInput: Stop here.\noutput: 저걸 가져.\n\n164\nInput: Take care.\noutput: 이거 가져.\n\n165\nInput: Take this.\noutput: 이거 가져.\n\n166\nInput: Take this.\noutput: 이거 가져.\n\n167\nInput: Take this.\noutput: 이거 가져.\n\n168\nInput: Thank you.\noutput: 그들이 사라.\n\n169\nInput: Then what?\noutput: 그 사람들은 아직 거야.\n\n170\nInput: They left.\noutput: 그 사람들은 아직 거야.\n\n171\nInput: They left.\noutput: 그 사람들은 아직 거야.\n\n172\nInput: They left.\noutput: 그 사람들은 아직 거야.\n\n173\nInput: They lied.\noutput: 그 사람들은 아직 거야.\n\n174\nInput: They lost.\noutput: 그 사람들은 아직 거야.\n\n175\nInput: They lost.\noutput: 그 사람들은 아직 거야.\n\n176\nInput: Tom cried.\noutput: 톰이 거짓말 했어.\n\n177\nInput: Tom dozed.\noutput: 톰이 거짓말 했어.\n\n178\nInput: Tom drove.\noutput: 톰이 거짓말 했어.\n\n179\nInput: Tom knits.\noutput: 톰이 거짓말 했어.\n\n180\nInput: Tom knows.\noutput: 톰이 거짓말 했어.\n\n181\nInput: Try again.\noutput: 이건 좀 해봐.\n\n182\nInput: Turn left.\noutput: 그들은 사라.\n\n183\nInput: Turn left.\noutput: 그들은 사라.\n\n184\nInput: Wait here.\noutput: 이걸 봐.\n\n185\nInput: Watch out!\noutput: 그만 말해!\n\n186\nInput: We talked.\noutput: 누가 이겼어?\n\n187\nInput: We waited.\noutput: 우린 기다.\n\n188\nInput: Well done!\noutput: 어서!\n\n189\nInput: Who cares?\noutput: 누가 좋아해?\n\n190\nInput: Who knows?\noutput: 누가 좋아해?\n\n191\nInput: Wonderful!\noutput: 이렇게 끔찍하다니!\n\n192\nInput: You idiot!\noutput: 우린 아직도 돼.\n\n193\nInput: You tried.\noutput: 우린 기로 거야.\n\n194\nInput: All aboard!\noutput: 이렇게 짜 수가!\n\n195\nInput: Ask anyone.\noutput: 이걸 좀 봐.\n\n196\nInput: Be careful.\noutput: 저걸 가져.\n\n197\nInput: Be patient.\noutput: 저걸 가져.\n\n198\nInput: Be patient.\noutput: 저걸 가져.\n\n199\nInput: Birds sing.\noutput: 이걸 조용해봐.\n\n"
        }
      ],
      "source": [
        "for idx in range(200):\n",
        "    print(idx)\n",
        "    input_seq = encoder_input_data[idx: idx+1]\n",
        "    decoded_sent = decode_seq(input_seq)\n",
        "    print('Input:', input_texts[idx])\n",
        "    print('output:', decoded_sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "venv",
      "display_name": "venv"
    },
    "colab": {
      "name": "NMT_seq2seq_basic.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}